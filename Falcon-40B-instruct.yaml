---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here: 
# https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects#criteria

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to http://opening-up-chatgpt.github.io
# and a citation to the paper in which the initial dataset & criteria were published:

# Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316 

system:
    name: Falcon-40B-instruct
    link: https://huggingface.co/tiiuae/falcon-40b-instruct
    type: text
    basemodelname: Falcon 40B
    endmodelname: Baize (synthetic)
    endmodellicense:  Apache 2.0 license
    notes:

org:
    name: Technology Innovation Institute
    link: https://falconllm.tii.ae
    notes:

# availability:
datasources:
    basemodel:
        class: partial
        link: https://huggingface.co/datasets/tiiuae/falcon-refinedweb
        notes: From the documentation 'The key ingredient for the high quality of the Falcon models is their training data, predominantly based (>80%) on RefinedWeb — a novel massive web dataset based on CommonCrawl' (https://huggingface.co/blog/falcon). However, only a small sample is made available.
    endmodel:
        class: partial
        link: https://github.com/project-baize/baize-chatbot
        notes: RL data inherited from Baize but provenance not well-documented. From the documentation 'Falcon-40B-Instruct was finetuned on a 150M tokens from Baize mixed with 5% of RefinedWeb data.'

weights:
    basemodel:
        class: open
        link: https://huggingface.co/tiiuae/falcon-40b-instruct/tree/main
        notes: Model weights available through HuggingFace library
    endmodel:
        class: closed
        link: https://github.com/project-baize/baize-chatbot#v1
        notes: No RL weights or checkpoints made available

trainingcode:
    class: closed
    link: https://huggingface.co/tiiuae/falcon-40b-instruct
    notes: No source code shared, even though the term "open source" is used.

# documentation
code:
    class: closed
    link:
    notes: No source code found, therefore no documentation found.

architecture:
    class: partial
    link:
    notes: Architecture sketched on HuggingFace as "Falcon-40B-Instruct is a 40B parameters causal decoder-only model built by TII based on Falcon-40B and finetuned on a mixture of Baize."

preprint:
    class: partial
    link: https://arxiv.org/abs/2306.01116
    notes: Preprint covers the creation and curation of RefinedWeb dataset, but not other aspects of the model, hence partial.

paper:
    class: closed
    link:
    notes: No peer-reviewed paper known.

modelcard:
    class: partial
    link: https://huggingface.co/tiiuae/falcon-40b-instruct
    notes: Model card on HuggingFace is mostly used to advertise the model, not to document its training and evaluation details.

datasheet:
    class: closed
    link:
    notes: There is no datasheet available.

# access
package:
    class: closed
    link:
    notes: There is no package.

api:
    class: closed
    link: https://huggingface.co/tiiuae/falcon-40b-instruct
    notes: There is no API, and HuggingFace inference API is disabled for this model.
    metaprompt: closed

licenses:
    class: open
    link:
    notes: First release came with a legally murky license that was swiftly criticised and now generates a 404. Current documentation 'Falcon-40B-Instruct is made available under the Apache 2.0 license.'
