---
# Thank you for contributing!
# In filling out this yaml file, please follow the criteria as described here: 
# https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/tree/main/projects#criteria

# You're free to build on this work and reuse the data. It is licensed under CC-BY 4.0, with the
# stipulation that attribution should come in the form of a link to http://opening-up-chatgpt.github.io
# and a citation to the paper in which the initial dataset & criteria were published:

# Liesenfeld, Andreas, Alianda Lopez, and Mark Dingemanse. 2023. “Opening up ChatGPT: Tracking Openness, Transparency, and Accountability in Instruction-Tuned Text Generators.” In CUI '23: Proceedings of the 5th International Conference on Conversational User Interfaces. July 19-21, Eindhoven. doi: 10.1145/3571884.3604316 

system:
    name: WizardLM 13B v1.2
    link: https://huggingface.co/WizardLM/WizardLM-13B-V1.2
    type: text 
    performanceclass: full
    basemodelname: LLaMA2-13B
    endmodelname: Evol-Instruct (synthetic)
    endmodellicense: Llama 2 license
    releasedate: 2023-07
    notes: Empowering Large Pre-Trained Language Models to Follow Complex Instructions

org:
    name: Microsoft & Peking University
    link: https://github.com/nlpxucan
    notes: 

# availability:
datasources_basemodel:
    class: closed
    link:  https://github.com/opening-up-chatgpt/opening-up-chatgpt.github.io/blob/main/projects/llama-2-chat.yaml
    notes: Based on LLaMA2, which is claimed to be public but nowhere exactly documented.
    
datasources_endmodel:
    class: open
    link: https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k
    notes: The Evol-Instruct V2 dataset contains 196k instruction-following sequences generated from Evol-Instruct. ShareGPT-4o, which is included in Evol-instruct, is gated behind sharing some personal information and agreeing to do no harm, but for the rest the dataset is fully open to inspection. The model card mentions recent changes in the open-source policy of the parent company leading to potential future issues with data sources.

weights_basemodel:
    class: partial
    link: https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    notes: Based on LLaMA2 weights, which are made conditionally available by Meta.

weights_endmodel:
    class: open
    link: https://huggingface.co/WizardLM/WizardLM-13B-V1.2
    notes: Model weights offered in HuggingFace repository

trainingcode:
    class: open
    link: https://github.com/nlpxucan/WizardLM/tree/main/WizardLM
    notes: 

# documentation:
code:
    class: open
    link: https://github.com/nlpxucan/WizardLM/tree/main/WizardLM
    notes: Code is comprehensively documented and contains demos.

architecture:
    class: open
    link: https://arxiv.org/abs/2304.12244
    notes: Architecture described in preprint and partly accessible in code repository

preprint:
    class: open
    link: https://arxiv.org/abs/2304.12244
    notes: Preprint describes method for creating large amounts of LLM-based synthetic RLHF data and fine-tuning WizardLM based on it

paper:
    class: closed
    link:
    notes: No peer-reviewed paper or data audit found. Preprint mentions it is 'under review'.

modelcard:
    class: closed
    link: https://huggingface.co/WizardLM/WizardLM-13B-V1.2
    notes: Model card is available, however contains links to pages talking about the model architecture, training, fine-tuning, and evaluation rather than containing them itself.

datasheet:
    class: closed
    link: https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k
    notes: Dataset card is available, but provides no information about data collection and curation. The preprint outlines data collection (based on a 52K instruction dataset from Alpaca) and curation at a high level.

# access:
package:
    class: closed
    link:
    notes: No package available. A package is available on PyPi, however its origin is dubious (https://github.com/pypi/support/issues/3178).

api:
    class: closed
    link:
    notes: No API available
    metaprompt: closed

licenses:
    class: partial
    link: https://github.com/nlpxucan/WizardLM/blob/main/WizardLM/MODEL_DIFF_LICENSE
    notes: Restricted for academic research purposes only. Code and Model diff release under CC-BY-NC-4.0, software code under Apache 2.0

